{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from path import Path as path\n",
    "from sklearn.model_selection import LeavePOut, cross_val_score, StratifiedKFold, permutation_test_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy.random import permutation\n",
    "from scipy.io import savemat, loadmat\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition des paramètres\n",
    "data_path = path('/home/tarek/Documents/arthur/Lab2/Loubna') # the path where the data is\n",
    "save_path = data_path / '../results' # path where saves are going to be made\n",
    "if not save_path.isdir(): # creates save dir if it does not exists\n",
    "    save_path.mkdir()\n",
    "    \n",
    "df = pd.read_csv(data_path / 'BD_17_11.csv') # loading data in pandas dataframe format\n",
    "\n",
    "rep_number = 100 # for unbalances classes : number of bootstraps\n",
    "n_permutations = 1000 # for permutation test\n",
    "subject_list = df['CODE'] # how to differentiate the subjects in the database : their code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tout est OK\n"
     ]
    }
   ],
   "source": [
    "''' A exécuter en deuxième à chaque fois'''\n",
    "def CondNames(cond):\n",
    "    '''permet de convertir les infos (1,2), (0) par exemple en infos intelligibles\n",
    "    dans notre cas (1,2) correspond à Parkinson, Démence\n",
    "    et (0) correspond à Disease Free\n",
    "    On fait donc dans l'exemple Conv vs DF '''\n",
    "    # ici on convertis les infos de la première colonne (indicée 0)\n",
    "    if cond[0][0] == cond[0][1]: # si les deux chiffres de la première colonne sont les mêmes\n",
    "                                 # On vérifie à quelle condition cela correspond\n",
    "        if cond[0][0] == 1:\n",
    "            name1 = 'Parkinson Disease'\n",
    "        elif cond[0][0] == 2:\n",
    "            name1 = 'Dementia Lewy bodies'\n",
    "        elif cond[0][0] == 0:\n",
    "            name1 = 'Disease Free'\n",
    "    else: # sinon, c'est qu'on a des chiffres différents donc on étudie les \"convertis\"\n",
    "        name1 = 'Converted'\n",
    "        \n",
    "    # ici on convertis les infos de la deuxième colonne (indicée 1)\n",
    "    if cond[1] == 0:\n",
    "        name2 = 'Disease Free'\n",
    "    elif cond[1] == 2:\n",
    "        name2 = 'Dementia Lewy bodies'\n",
    "    if cond[1] == 3:\n",
    "        name2 = 'Control'\n",
    "    return name1, name2\n",
    "\n",
    "def CreateLabels(dataset, cond):\n",
    "    # génère la liste des étiquettes en fonction du dataset et de la condition\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at = row['Type de Conversion']\n",
    "        if look_at == cond[0][0] or look_at == cond[0][1]:\n",
    "            label0_index.append(index)\n",
    "        elif look_at == cond[1]:\n",
    "            label1_index.append(index)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "# Ici on définit les conditions : ajouter ou supprimer des conditions\n",
    "conds_list = [#((2, 2), (3)), #  Démence vs Contrôle\n",
    "              ((1, 1), (2))] #  Parkinson vs Démence\n",
    "#               ((1, 1), (3)), #  PD vs COntrôle\n",
    "#               ((2, 2), (0)), #  Démence vs DF\n",
    "#               ((1, 1), (0)), #  Parkinson vs DF\n",
    "#               ((1, 2), (0)), #  Conv vs DF\n",
    "#               ((1, 2), (3)), #  Conv vs Contrôle\n",
    "#               ((0, 0), (3))] #  DF vs Contrôle\n",
    "\n",
    "# columns_to_drop = ['Conversion', 'PDvsDLB', 'DLBvs Ctrl', 'Type de Conversion']\n",
    "columns_to_drop += ['Age']\n",
    "MCI = False\n",
    "print('Tout est OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConditionsMCI(o):\n",
    "    name1, name2 = '',''\n",
    "    if o == 0:\n",
    "        name1, name2 = 'RBDMCI', 'RBDnoMCI'\n",
    "        conditions = [0,1,2]\n",
    "    elif o == 1:\n",
    "        name1, name2 = 'RBD', 'Control'\n",
    "        conditions = [0,1,2,3]\n",
    "    elif o == 2:\n",
    "        name1, name2 = 'RBDnoMCI + Control', 'RBDMCI'\n",
    "        conditions = [0,1,2,3]\n",
    "    return (name1, name2), conditions\n",
    "\n",
    "def CreateLabelsMCI(dataset, o):\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at2 = float(row['MCI au T1'])\n",
    "        look_at = row['Type de Conversion']\n",
    "        if o in (0,2):\n",
    "            if look_at2 == 1:\n",
    "                label0_index.append(index)\n",
    "            elif look_at2 == 0:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "        elif o == 1:\n",
    "            if look_at in (0,1,2):\n",
    "                label0_index.append(index)\n",
    "            elif look_at == 3:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "conds_list = ['trois', 'trucs', 'random']\n",
    "\n",
    "columns_to_drop = ['ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain']\n",
    "MCI = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset info for conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 features were dropped. Kept features :\n",
      "['Conversion', 'PDvsDLB', 'MCI au T1', 'ss-type MCI T1 (DxBrain)', 'Apnée', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'MMSE', 'Empandir', 'EmpanIndir', 'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'UPSITtotal12', 'UPDRS3']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 18 Parkinson Disease\n",
      "Il y a 26 features et 34 sujets.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    for o, cond in enumerate(conds_list):\n",
    "        dataset = df\n",
    "        kept_features = []\n",
    "        if MCI:\n",
    "            names, conditions = ConditionsMCI(o)\n",
    "        else:\n",
    "            names = CondNames(cond)\n",
    "            conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        # trie les sujets :\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        # on nettoie plus de sujets pour les conditions MCI\n",
    "        # nettoie les donnees :\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        # cree les labels :\n",
    "        if MCI:\n",
    "            label0_index, label1_index = CreateLabelsMCI(dataset, o)       \n",
    "            dataset = dataset.drop('MCI au T1', 1)\n",
    "        else:\n",
    "            label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "\n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "        # Verifier quelle est la classe minoritaire\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "#     columns_to_keep = [['TrailBris'] + ['Empanpon', 'MCI au T1', 'UPDRS3'],\n",
    "#                        ['Educ', 'Stroop43err','TrailBris'],\n",
    "#                        ['MCI au T1'],\n",
    "#                        ['TrailBris'],\n",
    "#                        ['ReyTot', 'ReyRI', 'Age'] + ['ReyB', 'EmpanIndir'],\n",
    "#                        ['Educ', 'Sex']]\n",
    "\n",
    "#     columns_to_keep = [['TrailBris', 'Empanpon' ]] # pour changer les features\n",
    "\n",
    "    rep_number = 5\n",
    "    n_permutations = 1000\n",
    "    \n",
    "    feature_list = ['MCI au T1', 'ss-type MCI T1 (DxBrain)',\n",
    "       'RBD_MCI single/multiple domain', 'Apnée', 'Age', 'Educ', 'Sex',\n",
    "       'PriseAnxio_AD', 'PriseAD', 'BDI', 'BAI', 'Epworth', 'MOCA', 'MMSE',\n",
    "       'Empandir', 'EmpanIndir', 'Empanpon', 'Stroop1_temps', 'Stroop1_err',\n",
    "       'Stroop2_temps', 'Stroop2_err', 'Stroop3_temps', 'Stroop3_erreur',\n",
    "       'Stroop4_temps', 'Stroop4_err', 'Stroop32temps', 'Stroop32err',\n",
    "       'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'Stroop43temps',\n",
    "       'Stroop43err', 'Flusem', 'Flupho', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD',\n",
    "       'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'Blocspon', 'Clom',\n",
    "       'SymptoRBD', 'UPSITtotal12', 'UPSITtotal40', 'UPSITpercexp',\n",
    "       'FM100total', 'FM100percnormal', 'UPDRS3']\n",
    "    \n",
    "    conds_list = [conds_list[0]] # pour selectionner la condition dans cond_list\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        kept_features = []\n",
    "        names = CondNames(cond)\n",
    "        dataset = df\n",
    "\n",
    "        columns_to_keep = ['CODE', 'Type de Conversion']\n",
    "        columns_to_keep.append(feature)\n",
    "        for column in dataset:\n",
    "            if column not in columns_to_keep:\n",
    "                try:\n",
    "                    dataset = dataset.drop(column, 1)\n",
    "                except:\n",
    "                    print('there was a problem droping', column)\n",
    "\n",
    "        conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "\n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "\n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            file_name = '%svs%s_with_selfeatures.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            if not file_path.isfile():\n",
    "                labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "                number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "                cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "                clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "                random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "                accuracies = []\n",
    "                first = True\n",
    "                pvalue = 0\n",
    "\n",
    "                len(dataset)\n",
    "                for perm in range(n_permutations+1):\n",
    "                    for my_set in random_sets:\n",
    "                        data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                        data = np.asarray(data)\n",
    "\n",
    "                        if first == True:\n",
    "                            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                            labels = np.asarray(labels, dtype=int)\n",
    "                            first = False\n",
    "\n",
    "                        clf = clf_choice\n",
    "                        accuracies.append(cross_val_score(clf, X=data, y=labels, cv=cv, n_jobs=-1).mean())\n",
    "\n",
    "                    labels = permutation(labels)\n",
    "                donnees = {'data':accuracies}\n",
    "                savemat(file_path, donnees)\n",
    "\n",
    "            else:\n",
    "                accuracies = loadmat(file_path)['data'].ravel()\n",
    "\n",
    "            pvalue = 0                \n",
    "            for score in accuracies[rep_number:]:\n",
    "                if score > np.mean(accuracies[:rep_number]):\n",
    "                    pvalue+=1/(n_permutations*rep_number)\n",
    "\n",
    "            print('%0.2f (+/-%0.2f) significatif a p=%0.4f\\n' % (np.mean(accuracies[:rep_number]), np.std(accuracies[:rep_number]), pvalue))\n",
    "\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')\n",
    "        o += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
