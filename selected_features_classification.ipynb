{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from path import Path as path\n",
    "from sklearn.model_selection import LeavePOut, cross_val_score, StratifiedKFold, permutation_test_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy.random import permutation\n",
    "from scipy.io import savemat, loadmat\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition des paramètres\n",
    "data_path = path('/home/tarek/Documents/arthur/Lab2/Loubna') # the path where the data is\n",
    "save_path = data_path / '../results' # path where saves are going to be made\n",
    "if not save_path.isdir(): # creates save dir if it does not exists\n",
    "    save_path.mkdir()\n",
    "    \n",
    "df = pd.read_csv(data_path / 'BD_17_11.csv') # loading data in pandas dataframe format\n",
    "\n",
    "rep_number = 100 # for unbalances classes : number of bootstraps\n",
    "n_permutations = 1000 # for permutation test\n",
    "subject_list = df['CODE'] # how to differentiate the subjects in the database : their code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tout est OK\n"
     ]
    }
   ],
   "source": [
    "''' A exécuter en deuxième à chaque fois'''\n",
    "def CondNames(cond):\n",
    "    '''permet de convertir les infos (1,2), (0) par exemple en infos intelligibles\n",
    "    dans notre cas (1,2) correspond à Parkinson, Démence\n",
    "    et (0) correspond à Disease Free\n",
    "    On fait donc dans l'exemple Conv vs DF '''\n",
    "    # ici on convertis les infos de la première colonne (indicée 0)\n",
    "    if cond[0][0] == cond[0][1]: # si les deux chiffres de la première colonne sont les mêmes\n",
    "                                 # On vérifie à quelle condition cela correspond\n",
    "        if cond[0][0] == 1:\n",
    "            name1 = 'Parkinson Disease'\n",
    "        elif cond[0][0] == 2:\n",
    "            name1 = 'Dementia Lewy bodies'\n",
    "        elif cond[0][0] == 0:\n",
    "            name1 = 'Disease Free'\n",
    "    else: # sinon, c'est qu'on a des chiffres différents donc on étudie les \"convertis\"\n",
    "        name1 = 'Converted'\n",
    "        \n",
    "    # ici on convertis les infos de la deuxième colonne (indicée 1)\n",
    "    if cond[1] == 0:\n",
    "        name2 = 'Disease Free'\n",
    "    elif cond[1] == 2:\n",
    "        name2 = 'Dementia Lewy bodies'\n",
    "    if cond[1] == 3:\n",
    "        name2 = 'Control'\n",
    "    return name1, name2\n",
    "\n",
    "def CreateLabels(dataset, cond):\n",
    "    # génère la liste des étiquettes en fonction du dataset et de la condition\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at = row['Type de Conversion']\n",
    "        if look_at == cond[0][0] or look_at == cond[0][1]:\n",
    "            label0_index.append(index)\n",
    "        elif look_at == cond[1]:\n",
    "            label1_index.append(index)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "# Ici on définit les conditions : ajouter ou supprimer des conditions\n",
    "conds_list = [#((2, 2), (3)), #  Démence vs Contrôle\n",
    "              ((1, 1), (2))] #  Parkinson vs Démence\n",
    "#               ((1, 1), (3)), #  PD vs COntrôle\n",
    "#               ((2, 2), (0)), #  Démence vs DF\n",
    "#               ((1, 1), (0)), #  Parkinson vs DF\n",
    "#               ((1, 2), (0)), #  Conv vs DF\n",
    "#               ((1, 2), (3)), #  Conv vs Contrôle\n",
    "#               ((0, 0), (3))] #  DF vs Contrôle\n",
    "\n",
    "columns_to_drop = ['Conversion', 'PDvsDLB', 'DLBvs Ctrl']\n",
    "columns_to_drop += ['Age']\n",
    "MCI = False\n",
    "print('Tout est OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConditionsMCI(o):\n",
    "    name1, name2 = '',''\n",
    "    if o == 0:\n",
    "        name1, name2 = 'RBDMCI', 'RBDnoMCI'\n",
    "        conditions = [0,1,2]\n",
    "    elif o == 1:\n",
    "        name1, name2 = 'RBD', 'Control'\n",
    "        conditions = [0,1,2,3]\n",
    "    elif o == 2:\n",
    "        name1, name2 = 'RBDnoMCI + Control', 'RBDMCI'\n",
    "        conditions = [0,1,2,3]\n",
    "    return (name1, name2), conditions\n",
    "\n",
    "def CreateLabelsMCI(dataset, o):\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at2 = float(row['MCI au T1'])\n",
    "        look_at = row['Type de Conversion']\n",
    "        if o in (0,2):\n",
    "            if look_at2 == 1:\n",
    "                label0_index.append(index)\n",
    "            elif look_at2 == 0:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "        elif o == 1:\n",
    "            if look_at in (0,1,2):\n",
    "                label0_index.append(index)\n",
    "            elif look_at == 3:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "conds_list = ['trois', 'trucs', 'random']\n",
    "\n",
    "columns_to_drop = ['ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain']\n",
    "MCI = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 features were dropped. Kept features :\n",
      "['TrailBris', 'ReyTot']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 18 Parkinson Disease\n",
      "Il y a 2 features et 34 sujets.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-397c3053b07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    o=0\n",
    "#     columns_to_keep = [['TrailBris'] + ['Empanpon', 'MCI au T1', 'UPDRS3'],\n",
    "#                        ['Educ', 'Stroop43err','TrailBris'],\n",
    "#                        ['MCI au T1'],\n",
    "#                        ['TrailBris'],\n",
    "#                        ['ReyTot', 'ReyRI', 'Age'] + ['ReyB', 'EmpanIndir'],\n",
    "#                        ['Educ', 'Sex']]\n",
    "\n",
    "#     columns_to_keep = [['TrailBris', 'Empanpon' ]] # pour changer les features\n",
    "\n",
    "    rep_number = 5\n",
    "    n_permutations = 1000\n",
    "    columns_to_keep = [['TrailBris', 'ReyTot']] # pour changer les features\n",
    "    conds_list = [conds_list[0]] # pour selectionner la condition dans cond_list\n",
    "    \n",
    "    for cond in conds_list:\n",
    "        kept_features = []\n",
    "        names = CondNames(cond)\n",
    "        dataset = df\n",
    "        \n",
    "        columns_to_keep[o] += ['CODE', 'Type de Conversion']\n",
    "        for column in dataset:\n",
    "            if column not in columns_to_keep[o]:\n",
    "                try:\n",
    "                    dataset = dataset.drop(column, 1)\n",
    "                except:\n",
    "                    print('there was a problem droping', column)\n",
    "                    \n",
    "        conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "    \n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "            \n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        \n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            file_name = '%svs%s_with_selfeatures.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            if not file_path.isfile():\n",
    "                labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "                number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "                cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "                clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "                random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "                accuracies = []\n",
    "                first = True\n",
    "                pvalue = 0\n",
    "\n",
    "                len(dataset)\n",
    "                for perm in range(n_permutations+1):\n",
    "                    for my_set in random_sets:\n",
    "                        data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                        data = np.asarray(data)\n",
    "\n",
    "                        if first == True:\n",
    "                            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                            labels = np.asarray(labels, dtype=int)\n",
    "                            first = False\n",
    "\n",
    "                        clf = clf_choice\n",
    "                        accuracies.append(cross_val_score(clf, X=data, y=labels, cv=cv, n_jobs=-1).mean())\n",
    "\n",
    "                    labels = permutation(labels)\n",
    "                donnees = {'data':accuracies}\n",
    "                savemat(file_path, donnees)\n",
    "                \n",
    "            else:\n",
    "                accuracies = loadmat(file_path)['data'].ravel()\n",
    "            \n",
    "            pvalue = 0                \n",
    "            for score in accuracies[rep_number:]:\n",
    "                if score > np.mean(accuracies[:rep_number]):\n",
    "                    pvalue+=1/(n_permutations*rep_number)\n",
    "\n",
    "            print('%0.2f (+/-%0.2f) significatif a p=%0.4f\\n' % (np.mean(accuracies[:rep_number]), np.std(accuracies[:rep_number]), pvalue))\n",
    "\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')\n",
    "        o += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
