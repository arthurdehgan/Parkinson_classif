{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from path import Path as path\n",
    "from sklearn.model_selection import LeavePOut, cross_val_score, StratifiedKFold, permutation_test_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy.random import permutation\n",
    "from scipy.io import savemat, loadmat\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition des paramètres\n",
    "data_path = path('/home/tarek/Documents/arthur/Lab2/Loubna') # the path where the data is\n",
    "save_path = data_path / '../results' # path where saves are going to be made\n",
    "if not save_path.isdir(): # creates save dir if it does not exists\n",
    "    save_path.mkdir()\n",
    "    \n",
    "df = pd.read_csv(data_path / 'BD_17_11.csv') # loading data in pandas dataframe format\n",
    "\n",
    "rep_number = 100 # for unbalances classes : number of bootstraps\n",
    "n_permutations = 1000 # for permutation test\n",
    "subject_list = df['CODE'] # how to differentiate the subjects in the database : their code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tout est OK\n"
     ]
    }
   ],
   "source": [
    "''' A exécuter en deuxième à chaque fois'''\n",
    "def CondNames(cond):\n",
    "    '''permet de convertir les infos (1,2), (0) par exemple en infos intelligibles\n",
    "    dans notre cas (1,2) correspond à Parkinson, Démence\n",
    "    et (0) correspond à Disease Free\n",
    "    On fait donc dans l'exemple Conv vs DF '''\n",
    "    # ici on convertis les infos de la première colonne (indicée 0)\n",
    "    if cond[0][0] == cond[0][1]: # si les deux chiffres de la première colonne sont les mêmes\n",
    "                                 # On vérifie à quelle condition cela correspond\n",
    "        if cond[0][0] == 1:\n",
    "            name1 = 'Parkinson Disease'\n",
    "        elif cond[0][0] == 2:\n",
    "            name1 = 'Dementia Lewy bodies'\n",
    "        elif cond[0][0] == 0:\n",
    "            name1 = 'Disease Free'\n",
    "    else: # sinon, c'est qu'on a des chiffres différents donc on étudie les \"convertis\"\n",
    "        name1 = 'Converted'\n",
    "        \n",
    "    # ici on convertis les infos de la deuxième colonne (indicée 1)\n",
    "    if cond[1] == 0:\n",
    "        name2 = 'Disease Free'\n",
    "    elif cond[1] == 2:\n",
    "        name2 = 'Dementia Lewy bodies'\n",
    "    if cond[1] == 3:\n",
    "        name2 = 'Control'\n",
    "    return name1, name2\n",
    "\n",
    "def CreateLabels(dataset, cond):\n",
    "    # génère la liste des étiquettes en fonction du dataset et de la condition\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at = row['Type de Conversion']\n",
    "        if look_at == cond[0][0] or look_at == cond[0][1]:\n",
    "            label0_index.append(index)\n",
    "        elif look_at == cond[1]:\n",
    "            label1_index.append(index)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "# Ici on définit les conditions : ajouter ou supprimer des conditions\n",
    "conds_list = [#((2, 2), (3)), #  Démence vs Contrôle\n",
    "              ((1, 1), (2))] #  Parkinson vs Démence\n",
    "#               ((1, 1), (3)), #  PD vs COntrôle\n",
    "#               ((2, 2), (0)), #  Démence vs DF\n",
    "#               ((1, 1), (0)), #  Parkinson vs DF\n",
    "#               ((1, 2), (0)), #  Conv vs DF\n",
    "#               ((1, 2), (3)), #  Conv vs Contrôle\n",
    "#               ((0, 0), (3))] #  DF vs Contrôle\n",
    "\n",
    "# columns_to_drop = ['Conversion', 'PDvsDLB', 'DLBvs Ctrl', 'Type de Conversion']\n",
    "columns_to_drop = ['Age']\n",
    "MCI = False\n",
    "print('Tout est OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConditionsMCI(o):\n",
    "    name1, name2 = '',''\n",
    "    if o == 0:\n",
    "        name1, name2 = 'RBDMCI', 'RBDnoMCI'\n",
    "        conditions = [0,1,2]\n",
    "    elif o == 1:\n",
    "        name1, name2 = 'RBD', 'Control'\n",
    "        conditions = [0,1,2,3]\n",
    "    elif o == 2:\n",
    "        name1, name2 = 'RBDnoMCI + Control', 'RBDMCI'\n",
    "        conditions = [0,1,2,3]\n",
    "    return (name1, name2), conditions\n",
    "\n",
    "def CreateLabelsMCI(dataset, o):\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at2 = float(row['MCI au T1'])\n",
    "        look_at = row['Type de Conversion']\n",
    "        if o in (0,2):\n",
    "            if look_at2 == 1:\n",
    "                label0_index.append(index)\n",
    "            elif look_at2 == 0:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "        elif o == 1:\n",
    "            if look_at in (0,1,2):\n",
    "                label0_index.append(index)\n",
    "            elif look_at == 3:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "conds_list = ['trois', 'trucs', 'random']\n",
    "\n",
    "columns_to_drop = ['ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain']\n",
    "MCI = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset info for conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    for o, cond in enumerate(conds_list):\n",
    "        dataset = df\n",
    "        kept_features = []\n",
    "        if MCI:\n",
    "            names, conditions = ConditionsMCI(o)\n",
    "        else:\n",
    "            names = CondNames(cond)\n",
    "            conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        # trie les sujets :\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        # on nettoie plus de sujets pour les conditions MCI\n",
    "        # nettoie les donnees :\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        # cree les labels :\n",
    "        if MCI:\n",
    "            label0_index, label1_index = CreateLabelsMCI(dataset, o)       \n",
    "            dataset = dataset.drop('MCI au T1', 1)\n",
    "        else:\n",
    "            label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "\n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "        # Verifier quelle est la classe minoritaire\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    rep_number = 100\n",
    "    for o, cond in enumerate(conds_list):\n",
    "        kept_features = []\n",
    "        dataset = df\n",
    "        \n",
    "        if MCI:\n",
    "            names, conditions = ConditionsMCI(o)\n",
    "        else:\n",
    "            names = CondNames(cond)\n",
    "            conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "#         if 3 in conditions:\n",
    "#             if 'MCI au T1' not in columns_to_drop:\n",
    "#                 columns_to_drop.append('MCI au T1')\n",
    "                \n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        if MCI:\n",
    "            label0_index, label1_index = CreateLabelsMCI(dataset, o)\n",
    "            dataset = dataset.drop('MCI au T1', 1)\n",
    "        else:\n",
    "            label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "            \n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        \n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        kept_features = np.asarray(kept_features)\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "            labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "            number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "            clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "            random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "            accuracies = []\n",
    "            first = True\n",
    "            pvalue = 0\n",
    "            file_name = '%svs%s_features.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            feats = []\n",
    "            if not file_path.isfile():\n",
    "                for my_set in random_sets:\n",
    "                    data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                    data = np.asarray(data)\n",
    "\n",
    "                    if first == True:\n",
    "                        labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                        labels = np.asarray(labels, dtype=int)\n",
    "                        first = False\n",
    "\n",
    "                    clf = clf_choice\n",
    "                    sfs1 = SFS(estimator=clf,\n",
    "                               k_features=(1,data.shape[1]),\n",
    "    #                            k_features=3,\n",
    "                               forward=False,\n",
    "                               floating=False,\n",
    "                               scoring='accuracy',\n",
    "                               verbose=1,\n",
    "                               cv=5,\n",
    "                               n_jobs=6)\n",
    "\n",
    "                    sfs1.fit(data, labels)\n",
    "                    print('Best combin (ACC= %.3f): %s' % (sfs1.k_score_, kept_features[list(sfs1.k_feature_idx_)].tolist()))\n",
    "                    feats.append(kept_features[list(sfs1.k_feature_idx_)].tolist())\n",
    "                sizes = [len(feat) for feat in feats]\n",
    "                for feat in feats:\n",
    "                    while len(feat) < max(sizes):\n",
    "                        feat.append('empty')\n",
    "                savemat(file_path, {'features': feats, 'score': sfs1.k_score_ })\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "for file in [f for f in save_path.files() if f.endswith('_features.mat')]:\n",
    "    a = loadmat(file)\n",
    "    features = a['features'].ravel()\n",
    "    for i, feature in enumerate(features):\n",
    "        features[i] = feature.replace(\" \", \"\")\n",
    "    scores = a['score']\n",
    "    del a\n",
    "    compte = {}.fromkeys(set(features),0)\n",
    "    for feature in features:\n",
    "        compte[feature] += 1\n",
    "    del compte['empty']\n",
    "    best_features = sorted(compte.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print(file.name[:-13].replace('vs', ' vs ') + ':', best_features, '\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
