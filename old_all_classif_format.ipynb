{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tout est OK\n"
     ]
    }
   ],
   "source": [
    "''' A exécuter en premier à chaque fois'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from path import Path as path\n",
    "from sklearn.model_selection import LeavePOut, cross_val_score, StratifiedKFold, permutation_test_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier as RF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy.random import permutation\n",
    "from scipy.io import savemat, loadmat\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "\n",
    "def CleanDataset(dataset, columns_to_drop, threshold=30):\n",
    "    '''Nettoie les données.'''\n",
    "    dropped_columns = []\n",
    "    dropped_subjects = []\n",
    "    subject_list = dataset['CODE']\n",
    "    for column in columns_to_drop:\n",
    "        if column in dataset.keys():\n",
    "            try:\n",
    "                dataset = dataset.drop(column, 1)\n",
    "                dropped_columns.append(column)\n",
    "            except:\n",
    "                print('there was a problem droping', column)\n",
    "                None\n",
    "            del columns_to_drop[columns_to_drop.index(column)]\n",
    "            \n",
    "    for thresh in reversed(range(1,threshold+1)):\n",
    "        dropped_counter = {}\n",
    "        for column in dataset:\n",
    "            for i in range(len(dataset)):\n",
    "                tested_value = dataset[column].iloc[i]\n",
    "                try:\n",
    "                    if type(tested_value) == str:\n",
    "                        tested_value = tested_value.replace(',', '.')\n",
    "                        a = float(tested_value)\n",
    "                        dataset.set_value(dataset.index.values[i], column, a)\n",
    "                except:\n",
    "                    if column not in columns_to_drop:\n",
    "                        columns_to_drop.append(column)\n",
    "                    break\n",
    "                if tested_value != tested_value or tested_value == float(-999):\n",
    "                    if column not in dropped_counter.keys():\n",
    "                        dropped_counter[column] = 1\n",
    "                    else:\n",
    "                        dropped_counter[column] += 1\n",
    "\n",
    "        for column in dropped_counter.keys():\n",
    "            if dropped_counter[column] >= thresh:\n",
    "                if column not in columns_to_drop:\n",
    "                    columns_to_drop.append(column)\n",
    "\n",
    "        for column in columns_to_drop:\n",
    "            if column in dataset.keys():\n",
    "                try:\n",
    "                    dataset = dataset.drop(column, 1)\n",
    "                    dropped_columns.append(column)\n",
    "                except:\n",
    "                    print('there was a problem droping', column)\n",
    "                    None\n",
    "                del columns_to_drop[columns_to_drop.index(column)]\n",
    "\n",
    "        subjects_to_drop = []\n",
    "        bad_subjects = {}\n",
    "        for index, row in dataset.iterrows():\n",
    "            for element in row:\n",
    "                if element != element:\n",
    "                    dataset = dataset.drop(index)\n",
    "                    break\n",
    "                elif element == float(-999):\n",
    "                    if subject_list[index] not in bad_subjects.keys():\n",
    "                        bad_subjects[subject_list[index]] = [1, index]\n",
    "                    else:\n",
    "                        bad_subjects[subject_list[index]][0] += 1\n",
    "        \n",
    "        nb_bad_subjects = len(bad_subjects.keys())\n",
    "        for subject in bad_subjects.keys():\n",
    "            if bad_subjects[subject][0] >= thresh:\n",
    "                subjects_to_drop.append(bad_subjects[subject][1])\n",
    "\n",
    "        for index in subjects_to_drop:\n",
    "            tested_subject = subject_list[index]\n",
    "#             try:\n",
    "#                 dataset = dataset.drop(index)\n",
    "#                 subject_list = subject_list.drop(index)\n",
    "#                 dropped_subjects.append(tested_subject)\n",
    "#             except:\n",
    "#                 print('there was a problem droping', tested_subject)\n",
    "    return dataset, dropped_columns, dropped_subjects\n",
    "\n",
    "\n",
    "def FindMinorClass(label0_index, label1_index):\n",
    "    # Création d'outils de balancing des classes\n",
    "    quantity_of_class = {'0': len(label0_index),\n",
    "                         '1': len(label1_index)}\n",
    "\n",
    "\n",
    "    nb_minority_class = min(quantity_of_class['0'], quantity_of_class['1'])\n",
    "\n",
    "    for key in quantity_of_class.keys():\n",
    "        if nb_minority_class == quantity_of_class[key]:\n",
    "            minority_class = int(key)\n",
    "\n",
    "    if minority_class == 0:\n",
    "        major_class = 1\n",
    "        minor_class_index = label0_index\n",
    "        major_class_index = label1_index\n",
    "    else:\n",
    "        major_class = 0\n",
    "        minor_class_index = label1_index\n",
    "        major_class_index = label0_index\n",
    "\n",
    "    minor_class_index = np.asarray(minor_class_index)\n",
    "    return minority_class, major_class, minor_class_index, major_class_index\n",
    "\n",
    "\n",
    "def CreateRandomBalancedDataset(dataset, minor_class_index, major_class_index, n_repet=1):\n",
    "    random_major_indexes = []\n",
    "    for i in range(n_repet):\n",
    "        my_set = np.random.choice(major_class_index, len(minor_class_index), replace=False)\n",
    "        random_major_indexes.append(my_set)\n",
    "    return random_major_indexes\n",
    "\n",
    "\n",
    "def SelectSubjects(dataset, conditions):\n",
    "    # sélectionne les sujets qui correpondent à nos conditions\n",
    "    cond = conditions\n",
    "    if 4 in cond:\n",
    "        cond.append(2)\n",
    "        cond.append(1)\n",
    "        del cond[cond.index(4)]\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at = row['Type de Conversion']\n",
    "        if look_at not in cond:\n",
    "            dataset = dataset.drop(index, 0)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Definition des paramètres\n",
    "data_path = path('.').abspath()#chemin d'Acces relatif vers absolu\n",
    "save_path = data_path / 'results'#doissier au nom de result sera crée pour sauvegarder des trucs\n",
    "if not save_path.isdir(): # crée le chemin de sauvegarde s'il nexiste pas\n",
    "    save_path.mkdir()\n",
    "# df = pd.read_csv(data_path + 'dataset.csv')\n",
    "df = pd.read_csv(data_path / 'BD_17_11.csv')\n",
    "\n",
    "\n",
    "rep_number = 100\n",
    "n_permutations = 1000\n",
    "subject_list = df['CODE']\n",
    "print('Tout est OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditions Originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tout est OK\n"
     ]
    }
   ],
   "source": [
    "''' A exécuter en deuxième à chaque fois'''\n",
    "def CondNames(cond):\n",
    "    '''permet de convertir les infos (1,2), (0) par exemple en infos intelligibles\n",
    "    dans notre cas (1,2) correspond à Parkinson, Démence\n",
    "    et (0) correspond à Disease Free\n",
    "    On fait donc dans l'exemple Conv vs DF '''\n",
    "    # ici on convertis les infos de la première colonne (indicée 0)\n",
    "    if cond[0][0] == cond[0][1]: # si les deux chiffres de la première colonne sont les mêmes\n",
    "                                 # On vérifie à quelle condition cela correspond\n",
    "        if cond[0][0] == 1:\n",
    "            name1 = 'Parkinson Disease'\n",
    "        elif cond[0][0] == 2:\n",
    "            name1 = 'Dementia Lewy bodies'\n",
    "        elif cond[0][0] == 0:\n",
    "            name1 = 'Disease Free'\n",
    "    else: # sinon, c'est qu'on a des chiffres différents donc on étudie les \"convertis\"\n",
    "        name1 = 'Converted'\n",
    "        \n",
    "    # ici on convertis les infos de la deuxième colonne (indicée 1)\n",
    "    if cond[1] == 0:\n",
    "        name2 = 'Disease Free'\n",
    "    elif cond[1] == 2:\n",
    "        name2 = 'Dementia Lewy bodies'\n",
    "    if cond[1] == 3:\n",
    "        name2 = 'Control'\n",
    "    return name1, name2\n",
    "\n",
    "def CreateLabels(dataset, cond):\n",
    "    # génère la liste des étiquettes en fonction du dataset et de la condition\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at = row['Type de Conversion']\n",
    "        if look_at == cond[0][0] or look_at == cond[0][1]:\n",
    "            label0_index.append(index)\n",
    "        elif look_at == cond[1]:\n",
    "            label1_index.append(index)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "# Ici on définit les conditions : ajouter ou supprimer des conditions\n",
    "conds_list = [#((2, 2), (3)), #  Démence vs Contrôle\n",
    "              ((1, 1), (2))] #  Parkinson vs Démence\n",
    "#               ((1, 1), (3)), #  PD vs COntrôle\n",
    "#               ((2, 2), (0)), #  Démence vs DF\n",
    "#               ((1, 1), (0)), #  Parkinson vs DF\n",
    "#               ((1, 2), (0)), #  Conv vs DF\n",
    "#               ((1, 2), (3)), #  Conv vs Contrôle\n",
    "#               ((0, 0), (3))] #  DF vs Contrôle\n",
    "\n",
    "# columns_to_drop = ['Conversion', 'PDvsDLB', 'DLBvs Ctrl', 'Type de Conversion']\n",
    "columns_to_drop = ['Age']\n",
    "MCI = False\n",
    "print('Tout est OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditions MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConditionsMCI(o):\n",
    "    name1, name2 = '',''\n",
    "    if o == 0:\n",
    "        name1, name2 = 'RBDMCI', 'RBDnoMCI'\n",
    "        conditions = [0,1,2]\n",
    "    elif o == 1:\n",
    "        name1, name2 = 'RBD', 'Control'\n",
    "        conditions = [0,1,2,3]\n",
    "    elif o == 2:\n",
    "        name1, name2 = 'RBDnoMCI + Control', 'RBDMCI'\n",
    "        conditions = [0,1,2,3]\n",
    "    return (name1, name2), conditions\n",
    "\n",
    "def CreateLabelsMCI(dataset, o):\n",
    "    label1_index = []\n",
    "    label0_index = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        look_at2 = float(row['MCI au T1'])\n",
    "        look_at = row['Type de Conversion']\n",
    "        if o in (0,2):\n",
    "            if look_at2 == 1:\n",
    "                label0_index.append(index)\n",
    "            elif look_at2 == 0:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "        elif o == 1:\n",
    "            if look_at in (0,1,2):\n",
    "                label0_index.append(index)\n",
    "            elif look_at == 3:\n",
    "                label1_index.append(index)\n",
    "            else:\n",
    "                dataset = dataset.drop(index, 0)\n",
    "    return label0_index, label1_index\n",
    "\n",
    "conds_list = ['trois', 'trucs', 'random']\n",
    "\n",
    "columns_to_drop = ['ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain']\n",
    "MCI = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset info for conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26 features were dropped. Kept features :\n",
      "['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'MMSE', 'Empandir', 'EmpanIndir', 'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'UPSITtotal12', 'UPSITpercexp', 'UPDRS3']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 18 Parkinson Disease\n",
      "Il y a 26 features et 34 sujets.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    for o, cond in enumerate(conds_list):\n",
    "        dataset = df\n",
    "        kept_features = []\n",
    "        if MCI:\n",
    "            names, conditions = ConditionsMCI(o)\n",
    "        else:\n",
    "            names = CondNames(cond)\n",
    "            conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        # trie les sujets :\n",
    "        dataset = SelectSubjects(dataset, conditions)s\n",
    "        # on nettoie plus de sujets pour les conditions MCI\n",
    "        # nettoie les donnees :\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        # cree les labels :\n",
    "        if MCI:\n",
    "            label0_index, label1_index = CreateLabelsMCI(dataset, o)       \n",
    "            dataset = dataset.drop('MCI au T1', 1)\n",
    "        else:\n",
    "            label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "\n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "        # Verifier quelle est la classe minoritaire\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "37 features were dropped. Kept features :\n",
      "['MCI au T1', 'Age', 'Educ', 'Sex', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'TrailB', 'TrailB_TrailA', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 32 Control\n",
      "Il y a 15 features et 48 sujets.\n",
      "0.88 (+/-0.04) significatif a p=0.0001\n",
      "\n",
      "\n",
      "38 features were dropped. Kept features :\n",
      "['MCI au T1', 'Age', 'Educ', 'Sex', 'Empandir', 'EmpanIndir', 'TrailA', 'TrailB', 'TrailB_TrailA', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "18 Parkinson Disease vs 32 Control\n",
      "Il y a 14 features et 50 sujets.\n",
      "0.65 (+/-0.06) significatif a p=0.0731\n",
      "\n",
      "\n",
      "29 features were dropped. Kept features :\n",
      "['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FReyco', 'UPSITtotal12', 'UPDRS3']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 42 Disease Free\n",
      "Il y a 23 features et 58 sujets.\n",
      "0.68 (+/-0.08) significatif a p=0.0566\n",
      "\n",
      "\n",
      "29 features were dropped. Kept features :\n",
      "['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FReyco', 'UPSITtotal12', 'UPDRS3']\n",
      "1 subjects were dropped :\n",
      "['T3032']\n",
      "17 Parkinson Disease vs 42 Disease Free\n",
      "Il y a 23 features et 59 sujets.\n",
      "0.52 (+/-0.10) significatif a p=0.4433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    for o, cond in enumerate(conds_list):\n",
    "        kept_features = []\n",
    "        dataset = df\n",
    "        \n",
    "        if MCI:\n",
    "            names, conditions = ConditionsMCI(o)\n",
    "        else:\n",
    "            names = CondNames(cond)\n",
    "            conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "#         if 3 in conditions:\n",
    "#             if 'MCI au T1' not in columns_to_drop:\n",
    "#                 columns_to_drop.append('MCI au T1')\n",
    "                \n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        if MCI:\n",
    "            label0_index, label1_index = CreateLabelsMCI(dataset, o)\n",
    "            dataset = dataset.drop('MCI au T1', 1)\n",
    "        else:\n",
    "            label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "            \n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        \n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            file_name = '%svs%s.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            if not file_path.isfile():\n",
    "                labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "                number_of_folds = max(int(nb_minority_class/2),2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "                cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "                #clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "                clf_choice= SVM() #ou mettre RF()  # C'EST ICI QUE JE CHANGE L'ALGO\n",
    "                \n",
    "                random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "                accuracies = []\n",
    "                first = True\n",
    "\n",
    "                len(dataset)\n",
    "                for perm in range(n_permutations):\n",
    "                    for my_set in random_sets:\n",
    "                        data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                        data = np.asarray(data)\n",
    "\n",
    "                        if first == True:\n",
    "                            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                            labels = np.asarray(labels, dtype=int)\n",
    "                            first = False\n",
    "\n",
    "                        clf = clf_choice\n",
    "                        accuracies.append(cross_val_score(clf, X=data, y=labels, cv=cv, n_jobs=6).mean())\n",
    "\n",
    "                    labels = permutation(labels)\n",
    "                donnees = {'data':accuracies}\n",
    "                savemat(file_path, donnees)\n",
    "                \n",
    "            else:\n",
    "                accuracies = loadmat(file_path)['data'].ravel()\n",
    "            \n",
    "            pvalue = 0\n",
    "            for score in accuracies[rep_number:]:\n",
    "                if score > np.mean(accuracies[:rep_number]):\n",
    "                    pvalue+=1/(n_permutations*rep_number)\n",
    "\n",
    "            print('%0.2f (+/-%0.2f) significatif a p=%0.4f\\n' % (np.mean(accuracies[:rep_number]), np.std(accuracies[:rep_number]), pvalue))\n",
    "\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classif Stroop Trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    o=0\n",
    "    # colonnes à garder, modifie ce que tu veux\n",
    "    columns_to_keep = ['Stroop1_temps', 'Stroop1_err', 'Stroop2_temps', \n",
    "                       'Stroop2_err', 'Stroop3_temps', 'Stroop3_erreur',\n",
    "                       'Stroop4_temps', 'Stroop4_err', 'Stroop32temps',\n",
    "                       'Stroop32err', 'TrailA', 'TrailB', 'TrailB_TrailA',\n",
    "                       'TrailBris', 'Stroop43temps', 'Stroop43err',\n",
    "                       'CODE', 'Type de Conversion'] # ne pas toucher cett ligne\n",
    "    for cond in conds_list:\n",
    "        kept_features = []\n",
    "        names = CondNames(cond)\n",
    "        dataset = df\n",
    "\n",
    "        for column in dataset:\n",
    "            if column not in columns_to_keep:\n",
    "                try:\n",
    "                    dataset = dataset.drop(column, 1)\n",
    "                except:\n",
    "                    print('there was a problem droping', column)\n",
    "                    \n",
    "        conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        \n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "            \n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        \n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            file_name = '%svs%s_stroop_trail.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            if not file_path.isfile():\n",
    "                labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "                number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "                cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "                clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "                random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "                print('%i %s vs %i %s' % (len(m_class_index), names[m_class], len(M_class_index), names[M_class]))\n",
    "\n",
    "                accuracies = []\n",
    "                first = True\n",
    "\n",
    "                len(dataset)\n",
    "                for perm in range(n_permutations+1):\n",
    "                    for my_set in random_sets:\n",
    "                        data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                        data = np.asarray(data)\n",
    "\n",
    "                        if first == True:\n",
    "                            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                            labels = np.asarray(labels, dtype=int)\n",
    "                            first = False\n",
    "\n",
    "                        clf = clf_choice\n",
    "                        accuracies.append(cross_val_score(clf, X=data, y=labels, cv=cv, n_jobs=-1).mean())\n",
    "\n",
    "                    labels = permutation(labels)\n",
    "                donnees = {'data':accuracies}\n",
    "                savemat(file_path, donnees)\n",
    "                \n",
    "            else:\n",
    "                accuracies = loadmat(file_path)['data'].ravel()\n",
    "\n",
    "            pvalue = 0                \n",
    "            for score in accuracies[rep_number:]:\n",
    "                if score > np.mean(accuracies[:rep_number]):\n",
    "                    pvalue+=1/(n_permutations*rep_number)\n",
    "\n",
    "            print('%0.2f (+/-%0.2f) significatif a p=%0.4f\\n' % (np.mean(accuracies[:rep_number]), np.std(accuracies[:rep_number]), pvalue))\n",
    "\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')\n",
    "        o += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27 features were dropped. Kept features :\n",
      "['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'UPSITtotal12', 'UPDRS3']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 18 Parkinson Disease\n",
      "Il y a 25 features et 34 sujets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.825): ['RBD_MCI single/multiple domain', 'Age', 'FigReyRI', 'UPDRS3', 'TrailBris', 'ReyTot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.842): ['MCI au T1', 'RBD_MCI single/multiple domain', 'Age', 'Sex', 'PriseAD', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'ReyTot', 'ReyB', 'FigReyRI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.817): ['MCI au T1', 'ReyRI', 'Sex', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.800): ['Age']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.875): ['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'EmpanIndir', 'TrailA', 'TrailB', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'UPSITtotal12']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.842): ['MCI au T1', 'RBD_MCI single/multiple domain', 'Age', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'Empandir', 'EmpanIndir', 'TrailA', 'ReyB', 'FigReyRD', 'UPSITtotal12']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.867): ['Age', 'Sex', 'FReyco', 'PriseAD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.875): ['MCI au T1', 'RBD_MCI single/multiple domain', 'Reyreco', 'Age', 'Sex', 'PriseAD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.908): ['UPDRS3', 'MCI au T1', 'Age', 'FReyco', 'PriseAD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.850): ['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'Age', 'Sex', 'PriseAD', 'EmpanIndir', 'TrailB', 'FigReyRI', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.867): ['ss-type MCI T1 (DxBrain)', 'Age', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.842): ['MCI au T1', 'RBD_MCI single/multiple domain', 'Age', 'Sex', 'PriseAD', 'Empandir', 'EmpanIndir', 'ReyB', 'FigReyRD', 'FReyco', 'UPSITtotal12']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.775): ['Age']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.817): ['MCI au T1', 'ReyB', 'Reyreco', 'UPSITtotal12', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'TrailBris']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.900): ['Age', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.825): ['Age', 'FigReyRI', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'FReyco', 'UPDRS3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.842): ['MCI au T1', 'RBD_MCI single/multiple domain', 'Age', 'Empandir', 'Empanpon', 'ReyRI', 'FigReyRI', 'FReyco', 'UPSITtotal12']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.900): ['Age', 'Sex', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.908): ['MCI au T1', 'EmpanIndir', 'Age', 'Sex', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.833): ['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'ReyB', 'ReyRD', 'FReyco']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.933): ['ReyB', 'MCI au T1', 'RBD_MCI single/multiple domain', 'Age', 'Empandir', 'Empanpon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.908): ['Age', 'Sex', 'FReyco', 'PriseAnxio_AD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.842): ['MCI au T1', 'ss-type MCI T1 (DxBrain)', 'RBD_MCI single/multiple domain', 'Age', 'Sex', 'PriseAD', 'Empandir', 'Empanpon', 'TrailB', 'TrailBris', 'FigReyRD', 'UPSITtotal12']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combin (ACC= 0.850): ['MCI au T1', 'Age', 'Empandir', 'EmpanIndir', 'Empanpon', 'TrailA', 'ReyRI', 'FReyco', 'UPDRS3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 1/25Process ForkPoolWorker-3714767:\n",
      "Process ForkPoolWorker-3714766:\n",
      "Process ForkPoolWorker-3714768:\n",
      "Process ForkPoolWorker-3714765:\n",
      "Process ForkPoolWorker-3714764:\n",
      "Process ForkPoolWorker-3714763:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tarek/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "catching classes that do not inherit from BaseException is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helping task handler/workers to finish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_help_stuff_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'removing tasks from inqueue until task handler finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b0fbba25b87a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                                n_jobs=6)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0msfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best combin (ACC= %.3f): %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_feature_idx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_feature_idx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    223\u001b[0m                         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                     )\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\u001b[0m in \u001b[0;36m_inclusion\u001b[0;34m(self, orig_set, subset, X, y)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mnew_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m                 \u001b[0mall_avg_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mall_cv_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\u001b[0m in \u001b[0;36m_calc_score\u001b[0;34m(self, X, y, indices)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                      \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                                      \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                                      pre_dispatch=self.pre_dispatch)\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mest_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;34m\"\"\"Shutdown the process or thread pool\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiprocessingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mWindowsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;31m# Workaround  occasional \"[Error 5] Access is denied\" issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0;31m# when trying to terminate a process under windows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: catching classes that do not inherit from BaseException is not allowed"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    rep_number = 100\n",
    "    for o, cond in enumerate(conds_list):\n",
    "        kept_features = []\n",
    "        dataset = df\n",
    "        \n",
    "        if MCI:\n",
    "            names, conditions = ConditionsMCI(o)\n",
    "        else:\n",
    "            names = CondNames(cond)\n",
    "            conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "#         if 3 in conditions:\n",
    "#             if 'MCI au T1' not in columns_to_drop:\n",
    "#                 columns_to_drop.append('MCI au T1')\n",
    "                \n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        if MCI:\n",
    "            label0_index, label1_index = CreateLabelsMCI(dataset, o)\n",
    "            dataset = dataset.drop('MCI au T1', 1)\n",
    "        else:\n",
    "            label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "            \n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        \n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        kept_features = np.asarray(kept_features)\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "            labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "            number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "            clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "            random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "            accuracies = []\n",
    "            first = True\n",
    "            pvalue = 0\n",
    "            file_name = '%svs%s_features.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            feats = []\n",
    "            if not file_path.isfile():\n",
    "                for my_set in random_sets:\n",
    "                    data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                    data = np.asarray(data)\n",
    "\n",
    "                    if first == True:\n",
    "                        labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                        labels = np.asarray(labels, dtype=int)\n",
    "                        first = False\n",
    "\n",
    "                    clf = clf_choice\n",
    "                    sfs1 = SFS(estimator=clf,\n",
    "                               k_features=(1,data.shape[1]),\n",
    "    #                            k_features=3,\n",
    "                               forward=False,\n",
    "                               floating=False,\n",
    "                               scoring='accuracy',\n",
    "                               verbose=1,\n",
    "                               cv=5,\n",
    "                               n_jobs=6)\n",
    "\n",
    "                    sfs1.fit(data, labels)\n",
    "                    print('Best combin (ACC= %.3f): %s' % (sfs1.k_score_, kept_features[list(sfs1.k_feature_idx_)].tolist()))\n",
    "                    feats.append(kept_features[list(sfs1.k_feature_idx_)].tolist())\n",
    "                sizes = [len(feat) for feat in feats]\n",
    "                for feat in feats:\n",
    "                    while len(feat) < max(sizes):\n",
    "                        feat.append('empty')\n",
    "                savemat(file_path, {'features': feats, 'score': sfs1.k_score_ })\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dementia Lewy bodies vs Control: [('MCIauT1', 100), ('Empandir', 49), ('Sex', 19), ('ReyRD', 16), ('Reyreco', 8), ('EmpanIndir', 7), ('Educ', 3), ('TrailB', 3), ('ReyTot', 2), ('ReyB', 2), ('TrailA', 2), ('Age', 1)] \n",
      "\n",
      "Dementia Lewy bodies vs Disease Free: [('UPSITtotal12', 71), ('PriseAnxio_AD', 64), ('MCIauT1', 45), ('Sex', 37), ('TrailBris', 24), ('RBD_MCIsingle/multipledomain', 22), ('Age', 21), ('Empandir', 20), ('PriseAD', 20), ('Reyreco', 18), ('TrailB', 17), ('EmpanIndir', 14), ('TrailA', 13), ('TrailB_TrailA', 11), ('ss-typeMCIT1(DxBrain)', 11), ('UPDRS3', 11), ('Educ', 11), ('ReyRI', 10), ('FReyco', 8), ('ReyRD', 7), ('Empanpon', 7), ('ReyB', 6), ('ReyTot', 5)] \n",
      "\n",
      "Parkinson Disease vs Disease Free: [('Sex', 48), ('Empanpon', 48), ('PriseAD', 46), ('TrailBris', 44), ('ReyRI', 42), ('MCIauT1', 38), ('RBD_MCIsingle/multipledomain', 33), ('UPDRS3', 32), ('ReyB', 30), ('PriseAnxio_AD', 29), ('ss-typeMCIT1(DxBrain)', 28), ('Reyreco', 28), ('Age', 28), ('ReyRD', 27), ('EmpanIndir', 26), ('ReyTot', 23), ('UPSITtotal12', 23), ('Empandir', 19), ('FReyco', 17), ('TrailA', 17), ('Educ', 16), ('TrailB_TrailA', 7), ('TrailB', 2)] \n",
      "\n",
      "Parkinson Disease vs Control: [('Age', 93), ('MCIauT1', 91), ('Reyreco', 72), ('EmpanIndir', 36), ('Sex', 34), ('Empandir', 32), ('ReyRI', 29), ('TrailA', 21), ('ReyTot', 17), ('ReyB', 17), ('TrailB_TrailA', 15), ('Educ', 14), ('ReyRD', 12), ('TrailB', 12)] \n",
      "\n",
      "Dementia Lewy bodies vs Parkinson Disease: [('Age', 76), ('MMSE', 67), ('MCIauT1', 56), ('Sex', 46), ('RBD_MCIsingle/multipledomain', 44), ('PriseAD', 40), ('PriseAnxio_AD', 38), ('Empandir', 29), ('ss-typeMCIT1(DxBrain)', 27), ('TrailB', 24), ('FigReyRD', 22), ('TrailBris', 21), ('FReyco', 20), ('EmpanIndir', 19), ('UPSITtotal12', 18), ('ReyB', 17), ('UPSITpercexp', 16), ('FigReyRI', 14), ('Reyreco', 13), ('UPDRS3', 11), ('TrailB_TrailA', 9), ('Educ', 9), ('ReyRI', 8), ('ReyRD', 5), ('ReyTot', 4), ('TrailA', 2)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "for file in [f for f in save_path.files() if f.endswith('_features.mat')]:\n",
    "    a = loadmat(file)\n",
    "    features = a['features'].ravel()\n",
    "    for i, feature in enumerate(features):\n",
    "        features[i] = feature.replace(\" \", \"\")\n",
    "    scores = a['score']\n",
    "    del a\n",
    "    compte = {}.fromkeys(set(features),0)\n",
    "    for feature in features:\n",
    "        compte[feature] += 1\n",
    "    del compte['empty']\n",
    "    best_features = sorted(compte.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print(file.name[:-13].replace('vs', ' vs ') + ':', best_features, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classif with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    o=0\n",
    "#     columns_to_keep = [['TrailBris'] + ['Empanpon', 'MCI au T1', 'UPDRS3'],\n",
    "#                        ['Educ', 'Stroop43err','TrailBris'],\n",
    "#                        ['MCI au T1'],\n",
    "#                        ['TrailBris'],\n",
    "#                        ['ReyTot', 'ReyRI', 'Age'] + ['ReyB', 'EmpanIndir'],\n",
    "#                        ['Educ', 'Sex']]\n",
    "\n",
    "#     columns_to_keep = [['TrailBris', 'Empanpon' ]] # pour changer les features\n",
    "\n",
    "    rep_number = 5\n",
    "    n_permutations = 1000\n",
    "    columns_to_keep = [['TrailBris', 'ReyTot']] # pour changer les features\n",
    "    conds_list = [conds_list[0]] # pour selectionner la condition dans cond_list\n",
    "    \n",
    "    for cond in conds_list:\n",
    "        kept_features = []\n",
    "        names = CondNames(cond)\n",
    "        dataset = df\n",
    "        \n",
    "        columns_to_keep[o] += ['CODE', 'Type de Conversion']\n",
    "        for column in dataset:\n",
    "            if column not in columns_to_keep[o]:\n",
    "                try:\n",
    "                    dataset = dataset.drop(column, 1)\n",
    "                except:\n",
    "                    print('there was a problem droping', column)\n",
    "                    \n",
    "        conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "    \n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "            \n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "        \n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            file_name = '%svs%s_with_selfeatures.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            if not file_path.isfile():\n",
    "                labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "                number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "                cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "                clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "                random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "                accuracies = []\n",
    "                first = True\n",
    "                pvalue = 0\n",
    "\n",
    "                len(dataset)\n",
    "                for perm in range(n_permutations+1):\n",
    "                    for my_set in random_sets:\n",
    "                        data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                        data = np.asarray(data)\n",
    "\n",
    "                        if first == True:\n",
    "                            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                            labels = np.asarray(labels, dtype=int)\n",
    "                            first = False\n",
    "\n",
    "                        clf = clf_choice\n",
    "                        accuracies.append(cross_val_score(clf, X=data, y=labels, cv=cv, n_jobs=-1).mean())\n",
    "\n",
    "                    labels = permutation(labels)\n",
    "                donnees = {'data':accuracies}\n",
    "                savemat(file_path, donnees)\n",
    "                \n",
    "            else:\n",
    "                accuracies = loadmat(file_path)['data'].ravel()\n",
    "            \n",
    "            pvalue = 0                \n",
    "            for score in accuracies[rep_number:]:\n",
    "                if score > np.mean(accuracies[:rep_number]):\n",
    "                    pvalue+=1/(n_permutations*rep_number)\n",
    "\n",
    "            print('%0.2f (+/-%0.2f) significatif a p=%0.4f\\n' % (np.mean(accuracies[:rep_number]), np.std(accuracies[:rep_number]), pvalue))\n",
    "\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')\n",
    "        o += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each feature one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'columns_to_keep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a5a95056c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CODE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Type de Conversion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'columns_to_keep' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "#     columns_to_keep = [['TrailBris'] + ['Empanpon', 'MCI au T1', 'UPDRS3'],\n",
    "#                        ['Educ', 'Stroop43err','TrailBris'],\n",
    "#                        ['MCI au T1'],\n",
    "#                        ['TrailBris'],\n",
    "#                        ['ReyTot', 'ReyRI', 'Age'] + ['ReyB', 'EmpanIndir'],\n",
    "#                        ['Educ', 'Sex']]\n",
    "\n",
    "#     columns_to_keep = [['TrailBris', 'Empanpon' ]] # pour changer les features\n",
    "\n",
    "    rep_number = 5\n",
    "    n_permutations = 1000\n",
    "    \n",
    "    feature_list = ['MCI au T1', 'ss-type MCI T1 (DxBrain)',\n",
    "       'RBD_MCI single/multiple domain', 'Apnée', 'Age', 'Educ', 'Sex',\n",
    "       'PriseAnxio_AD', 'PriseAD', 'BDI', 'BAI', 'Epworth', 'MOCA', 'MMSE',\n",
    "       'Empandir', 'EmpanIndir', 'Empanpon', 'Stroop1_temps', 'Stroop1_err',\n",
    "       'Stroop2_temps', 'Stroop2_err', 'Stroop3_temps', 'Stroop3_erreur',\n",
    "       'Stroop4_temps', 'Stroop4_err', 'Stroop32temps', 'Stroop32err',\n",
    "       'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'Stroop43temps',\n",
    "       'Stroop43err', 'Flusem', 'Flupho', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD',\n",
    "       'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'Blocspon', 'Clom',\n",
    "       'SymptoRBD', 'UPSITtotal12', 'UPSITtotal40', 'UPSITpercexp',\n",
    "       'FM100total', 'FM100percnormal', 'UPDRS3']\n",
    "    \n",
    "    conds_list = [conds_list[0]] # pour selectionner la condition dans cond_list\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        kept_features = []\n",
    "        names = CondNames(cond)\n",
    "        dataset = df\n",
    "\n",
    "        columns_to_keep = ['CODE', 'Type de Conversion']\n",
    "        columns_to_keep.append(feature)\n",
    "        for column in dataset:\n",
    "            if column not in columns_to_keep:\n",
    "                try:\n",
    "                    dataset = dataset.drop(column, 1)\n",
    "                except:\n",
    "                    print('there was a problem droping', column)\n",
    "\n",
    "        conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "        dataset = SelectSubjects(dataset, conditions)\n",
    "        dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "        label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "        dataset = dataset.drop('Type de Conversion', 1)\n",
    "\n",
    "        for column in dataset:\n",
    "            kept_features.append(column)\n",
    "\n",
    "        print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "        print(kept_features)\n",
    "        print(len(dropped_subjects), 'subjects were dropped :')\n",
    "        print(dropped_subjects)\n",
    "\n",
    "        m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "        nb_minority_class = len(m_class_index)\n",
    "\n",
    "        print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "        if nb_minority_class > 4:\n",
    "            print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "            file_name = '%svs%s_with_selfeatures.mat' % (names[m_class], names[M_class])\n",
    "            file_path = save_path / file_name\n",
    "            if not file_path.isfile():\n",
    "                labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "                number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "                cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "                clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "                random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "                accuracies = []\n",
    "                first = True\n",
    "                pvalue = 0\n",
    "\n",
    "                len(dataset)\n",
    "                for perm in range(n_permutations+1):\n",
    "                    for my_set in random_sets:\n",
    "                        data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                        data = np.asarray(data)\n",
    "\n",
    "                        if first == True:\n",
    "                            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                            labels = np.asarray(labels, dtype=int)\n",
    "                            first = False\n",
    "\n",
    "                        clf = clf_choice\n",
    "                        accuracies.append(cross_val_score(clf, X=data, y=labels, cv=cv, n_jobs=-1).mean())\n",
    "\n",
    "                    labels = permutation(labels)\n",
    "                donnees = {'data':accuracies}\n",
    "                savemat(file_path, donnees)\n",
    "\n",
    "            else:\n",
    "                accuracies = loadmat(file_path)['data'].ravel()\n",
    "\n",
    "            pvalue = 0                \n",
    "            for score in accuracies[rep_number:]:\n",
    "                if score > np.mean(accuracies[:rep_number]):\n",
    "                    pvalue+=1/(n_permutations*rep_number)\n",
    "\n",
    "            print('%0.2f (+/-%0.2f) significatif a p=%0.4f\\n' % (np.mean(accuracies[:rep_number]), np.std(accuracies[:rep_number]), pvalue))\n",
    "\n",
    "        else:\n",
    "            print('Not enough subjects to perform classification\\n')\n",
    "        o += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29 features were dropped. Kept features :\n",
      "['Conversion', 'PDvsDLB', 'MCI au T1', 'ss-type MCI T1 (DxBrain)', 'Apnée', 'Educ', 'Sex', 'PriseAnxio_AD', 'PriseAD', 'MMSE', 'Empandir', 'EmpanIndir', 'TrailA', 'TrailB', 'TrailB_TrailA', 'TrailBris', 'ReyTot', 'ReyB', 'ReyRI', 'ReyRD', 'Reyreco', 'FigReyRI', 'FigReyRD', 'FReyco', 'UPSITtotal12', 'UPSITpercexp', 'UPDRS3']\n",
      "0 subjects were dropped :\n",
      "[]\n",
      "16 Dementia Lewy bodies vs 18 Parkinson Disease\n",
      "Il y a 27 features et 34 sujets.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7d7201d52ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                         \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarek/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Pour classif Parkinson-Démence :\n",
    "    # DLB=2 PD=1 Normal=0 Contrôle=3\n",
    "    cond = conds_list[0]\n",
    "    kept_features = []\n",
    "    names = CondNames(cond)\n",
    "    dataset = df\n",
    "    \n",
    "    conditions = [cond[0][0], cond[0][1], cond[1]]\n",
    "    dataset = SelectSubjects(dataset, conditions)\n",
    "    dataset, dropped_columns, dropped_subjects = CleanDataset(dataset, columns_to_drop)\n",
    "    label0_index, label1_index = CreateLabels(dataset, cond)\n",
    "    dataset = dataset.drop('Type de Conversion', 1)\n",
    "    \n",
    "    for column in dataset:\n",
    "        kept_features.append(column)\n",
    "\n",
    "    print('\\n%s features were dropped. Kept features :' % len(dropped_columns))\n",
    "    print(kept_features)\n",
    "    print(len(dropped_subjects), 'subjects were dropped :')\n",
    "    print(dropped_subjects)\n",
    "\n",
    "    m_class, M_class, m_class_index, M_class_index = FindMinorClass(label0_index, label1_index)\n",
    "    nb_minority_class = len(m_class_index)\n",
    "\n",
    "    print('%i %s vs %i %s' % (nb_minority_class, names[m_class], len(M_class_index), names[M_class]))\n",
    "    if nb_minority_class > 4:\n",
    "        print(\"Il y a %i features et %i sujets.\" % (dataset.shape[1], len(dataset)))\n",
    "        file_name = '%svs%s_single_feature.mat' % (names[m_class], names[M_class])\n",
    "        file_path = save_path / file_name\n",
    "        if not file_path.isfile():\n",
    "            labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "            labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "            number_of_folds = int(nb_minority_class/2)  # aura pour équivalent Leave 4 subject Out stratifié\n",
    "            cv = StratifiedKFold(n_splits=number_of_folds, shuffle=True)\n",
    "            clf_choice = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200, learning_rate=1)\n",
    "\n",
    "            random_sets = CreateRandomBalancedDataset(dataset, m_class_index, M_class_index, rep_number)\n",
    "\n",
    "            accuracies = []\n",
    "            first = True\n",
    "            accuracies = np.zeros((len(kept_features), n_permutations, len(random_sets)))\n",
    "            for perm in range(n_permutations):\n",
    "                for i in range(len(random_sets)):\n",
    "                    my_set = random_sets[i]\n",
    "                    data = pd.concat([dataset.loc[my_set], dataset.loc[m_class_index]])\n",
    "                    data = np.asarray(data)\n",
    "\n",
    "                    if first == True:\n",
    "                        labels = [M_class]*nb_minority_class + [m_class]*nb_minority_class\n",
    "                        labels = np.asarray(labels, dtype=int)\n",
    "                        first = False\n",
    "\n",
    "                    for k in range(data.shape[1]):\n",
    "                        clf = clf_choice\n",
    "                        accuracies[k, perm, i] = cross_val_score(clf, X=data[:,k].reshape(-1, 1), y=labels, cv=cv, n_jobs=-1).mean()\n",
    "\n",
    "                labels = permutation(labels)\n",
    "                \n",
    "            donnees = {'data':accuracies}\n",
    "            savemat(file_path, donnees)\n",
    "        else:\n",
    "            accuracies = loadmat(file_path)['data']\n",
    "            \n",
    "        for i, feature in enumerate(kept_features):\n",
    "            pvalue = 0\n",
    "            for perm in range(1, accuracies.shape[1]):\n",
    "                for score in accuracies[i, perm, :]:\n",
    "                    if score > accuracies[i, 0, :].mean():\n",
    "                        pvalue+=1/5000\n",
    "            print('%s : %0.2f (+/-%0.2f) significatif a p=%0.4f' % (feature, accuracies[i, 0, :].mean(), accuracies[i, 0, :].std(), pvalue))\n",
    "\n",
    "    else:\n",
    "        print('Not enough subjects to perform classification\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
